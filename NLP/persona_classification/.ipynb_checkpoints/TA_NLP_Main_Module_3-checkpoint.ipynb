{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32919468-b2e1-4614-8430-6acff7b9f03c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3a265a-1330-4b25-8dc6-7722d1a2e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dracbook/devroot/python/cs605_proj_py3_10/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.37.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import IPython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer,  DataCollatorWithPadding\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login()\n",
    "\n",
    "print(IPython.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5991be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "MPS available: True\n",
      "MPS device is available and selected.\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available and selected.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not available. Using CPU.\")\n",
    "\n",
    "x = torch.tensor([1, 2, 3], device=device)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f476661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19066313-940d-44b6-883d-b3c8b134acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
     ]
    }
   ],
   "source": [
    "# All spaCy labels\n",
    "print(nlp.get_pipe(\"ner\").labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bdbf2-baa7-4708-a63a-05a971159947",
   "metadata": {},
   "source": [
    "| **Label**  | **Purpose in Your Agent**                                                      |\n",
    "| ---------- | ------------------------------------------------------------------------------ |\n",
    "| `GPE`      | Detect cities, countries, districts (e.g., *Singapore*, *Kampong Glam*)        |\n",
    "| `LOC`      | Identify general locations (e.g., *Marina Bay*, *Orchard Road*)                |\n",
    "| `FAC`      | Capture facilities like *MRT*, *airport*, *hotel*                              |\n",
    "| `DATE`     | Recognize travel dates (e.g., *3 June*, *next Monday*)                         |\n",
    "| `TIME`     | Times of day for activities or bookings (e.g., *10 AM*, *evening*)             |\n",
    "| `DURATION` | Trip length or durations (e.g., *3 days*, *2 nights*)                          |\n",
    "| `ORG`      | Travel operators, hotel chains, airlines (e.g., *Expedia*, *Marina Bay Sands*) |\n",
    "| `MONEY`    | Budget, pricing (e.g., *SGD 100*, *\\$200*)                                     |\n",
    "| `PERSON`   | User or people mentioned in dialogue (for chatbot personalization if needed)   |\n",
    "| `EVENT`    | Named events or festivals (e.g., *Singapore Night Festival*)                   |\n",
    "| `CARDINAL` | Generic numbers (e.g., *2 adults*, *4 attractions*)                            |\n",
    "| `ORDINAL`  | Day number in trip or itinerary step (e.g., *first day*, *3rd night*)          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22859747-e0ef-4600-a055-7c29fb0039a6",
   "metadata": {},
   "source": [
    "# NLP Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0019a76-8c1d-466d-9708-3e9bfd723dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based dictionary of known terms\n",
    "FOOD_TERMS = {'rojak', 'prawn mee', 'bubur cha cha', 'mee siam', 'itek-itek', 'popiah', \n",
    "              'fish head curry', 'pongal', 'kueh pie tee', 'char kway teow', \n",
    "              'hainanese chicken rice', 'mee goreng', 'wonton mee', 'nasi lemak', \n",
    "              'beef rendang', 'bakuteh', 'nasi padang', 'teochew porridge', \n",
    "              'yong tau foo', 'char koay teow', 'kueh salat', 'bak chor mee', 'chicken wings', \n",
    "              'curry puffs', 'kong bah pau', 'oyster omelette', 'bak kut teh', 'har jeong gai', \n",
    "              'kway chap', 'mee rebus ayam', 'laksa', 'mee rebus', 'wan tan mee', 'otah-otah', \n",
    "              'carrot cake', 'ayam buah keluak', 'satay', 'lor mee'}\n",
    "\n",
    "TRANSPORT_TERMS = {\"mrt\", \"ez-link\", \"bus pass\", \"circle line\", \"east west line\"}\n",
    "LOCATION_TERMS = {\"marina bay\", \"kampong glam\", \"chinatown\", \"sentosa\"}\n",
    "\n",
    "SPECIAL_REQUIREMENT_TERMS = [\n",
    "    # Accessibility\n",
    "    \"wheelchair\", \"disabled\", \"elderly\", \"mobility\", \"ramp\", \"accessible\", \"no stairs\",\n",
    "    \n",
    "    # Dietary\n",
    "    \"halal\", \"vegetarian\", \"vegan\", \"gluten-free\", \"kosher\",\n",
    "    \n",
    "    # Kid/baby-friendly\n",
    "    \"stroller\", \"baby\", \"infant\", \"kid-friendly\", \"child seat\",\n",
    "    \n",
    "    # Pet-related\n",
    "    \"pet-friendly\", \"pets allowed\", \"dog\", \"cat\", \"no pets\",\n",
    "\n",
    "    # Sensory/environmental\n",
    "    \"quiet\", \"no smoking\", \"non-smoking\", \"low crowd\", \"avoid crowded\", \"no stairs\", \"no noise\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b526c7fb-a8e3-4675-a153-7e9fa5bc9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define essential fields that must be filled\n",
    "ESSENTIAL_FIELDS = [\"intent\", \"location\", \"date\", \"duration_days\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62545-c6ac-4f07-8052-cb6392badfb5",
   "metadata": {},
   "source": [
    "## Load Trained Intent Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e2d46a-dc4a-4def-b360-2cf47ca4925b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model and tokenizer\n",
    "model_path = \"./intent_model\"  # Change to your model directory\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3aa6e3-6e06-4c21-b3a0-1cde312b2471",
   "metadata": {},
   "source": [
    "## Classify Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd1e542-9c4d-4a78-840d-a573868f366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'FindPlace': 0, 'BookFlight': 1, 'AskOpeningHour': 2, 'SearchHotel': 3, 'PlanItinerary': 4}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a58b5e-fecc-4019-a1ab-263bb36fec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return id2label[pred_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3a6ff-0c63-4f8f-8bf2-29dde9ef202d",
   "metadata": {},
   "source": [
    "## Understand the Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7a1616-4b14-4909-9bcd-55286706da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Render dependencies\n",
    "    dep_html = displacy.render(doc, style=\"dep\", page=True)\n",
    "    display(HTML(dep_html))\n",
    "\n",
    "    # Render named entities\n",
    "    ent_html = displacy.render(doc, style=\"ent\")\n",
    "    display(HTML(ent_html))\n",
    "\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:15} | Lemma: {token.lemma_:10} | POS: {token.pos_:8} | Tag: {token.tag_:6} | Dep: {token.dep_:12} | \"\n",
    "        f\"Shape: {token.shape_:10} | Alpha: {token.is_alpha} | Stop: {token.is_stop}\")\n",
    "\n",
    "    return \"Dependency tree saved to 'dep_tree.html', Entity visualization saved to 'entities.html'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a7749-11d1-4afc-8c76-9435fa378ed4",
   "metadata": {},
   "source": [
    "## Initial the Dialogue State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0023aa94-9b87-4656-97e5-f140d4ecb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base dialogue state format\n",
    "def init_dialogue_state():\n",
    "    return {\n",
    "          \"intent\": None,\n",
    "          \"location\": [],\n",
    "          \"date\": [],\n",
    "          \"duration_days\": [],\n",
    "          \"food\": [],\n",
    "          \"budget\": [],\n",
    "          \"transport\": [],\n",
    "          \"event\": [],\n",
    "          \"style\": [],                   \n",
    "          \"num_kids\": [],\n",
    "          \"num_adults\": [],\n",
    "          \"special\": [],\n",
    "    }\n",
    "\n",
    "# x Destination: Singapore, Sentosa\n",
    "# Persona: family with kids, solo traveler\n",
    "# Activity: shopping, food, studying\n",
    "# Accommodation: 4-star hotel, budget hotel\n",
    "# x Transport: flight, car, train, cruise, ferry\n",
    "# x Duration: 3 days, two weeks\n",
    "# x Date: July, 10 June 2025\n",
    "# x Scope (Intent): overall trip planning, accommodation advice, food advice\n",
    "# Tip: best time to visit, weather in December\n",
    "# x Budget: under $1000\", luxurious\n",
    "# Custom: visa, passport validity\n",
    "# more..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa9691-fcb6-46b7-b7bb-3f438f7d9de1",
   "metadata": {},
   "source": [
    "## Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a43a7291-f352-4a9e-8868-cce22996886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping spaCy labels to our labels\n",
    "spacy_to_custom_labels = {\n",
    "    \"GPE\": \"location\",\n",
    "    \"LOC\": \"location\",\n",
    "    \"FAC\": \"location\",\n",
    "    \"DATE\": \"date\",\n",
    "    \"TIME\": \"date\",\n",
    "    \"DURATION\": \"date\",\n",
    "    \"EVENT\": \"event\",\n",
    "    \"MONEY\": \"budget\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1b670-e553-4676-981d-2a5e98197923",
   "metadata": {},
   "source": [
    "### Time Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b89cff-b6a3-4398-9c3c-6eabc9e925d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Initialize (load once)\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(\"./frequency_dictionary_en_82_765.txt\", 0, 1)\n",
    "\n",
    "def correct_text(text):\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96720cc4-0653-4039-b44c-e562f31a7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "def fuzzy_match_time_unit(word):\n",
    "    time_units = [\"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\", \"night\", \"nights\"]\n",
    "    result = process.extractOne(word, time_units, scorer=fuzz.ratio)\n",
    "    if result:\n",
    "        match, score = result[0], result[1]\n",
    "        return match if score >= 80 else None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4bcb83c-2f79-4ba1-95e7-2da22ecbda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_time_entity_fuzzy(ent):\n",
    "    import re\n",
    "\n",
    "    text = ent.text.lower().strip()\n",
    "    corrected = correct_text(text)\n",
    "\n",
    "    # Patterns\n",
    "    month_keywords = [\n",
    "        \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "        \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "        \"jan\", \"feb\", \"mar\", \"apr\", \"jun\", \"jul\", \"aug\", \"sep\", \"sept\", \"oct\", \"nov\", \"dec\"\n",
    "    ]\n",
    "\n",
    "    weekday_keywords = [\n",
    "        \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\",\n",
    "        \"mondays\", \"tuesdays\", \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\"\n",
    "    ]\n",
    "\n",
    "    # If it looks like a range, treat it as date\n",
    "    if re.search(r\"\\b(from|to|until|between)\\b\", corrected):\n",
    "        return \"date\"\n",
    "\n",
    "    # If contains explicit duration unit like \"days\", \"weeks\"\n",
    "    unit = fuzzy_match_time_unit(corrected.split()[-1])\n",
    "    if unit and any(char.isdigit() for char in corrected):\n",
    "        return \"duration_days\"\n",
    "\n",
    "    if any(month in corrected for month in month_keywords):\n",
    "        return \"date\"\n",
    "\n",
    "    if any(day in corrected for day in weekday_keywords):\n",
    "        return \"date\"\n",
    "\n",
    "    # Detect YYYY-MM-DD or DD/MM/YYYY patterns\n",
    "    if re.search(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\", corrected) or re.search(r\"\\d{4}-\\d{2}-\\d{2}\", corrected):\n",
    "        return \"date\"\n",
    "\n",
    "    # Fallback to original label\n",
    "    return ent.label_.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "523d9fd1-a6a7-4edd-8f26-bcd3ad2660ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def normalize_date(text):\n",
    "    try:\n",
    "        dt = parser.parse(text, fuzzy=True, dayfirst=True)\n",
    "        return dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd32108-c521-45fe-9602-e32f5966aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_potential_noise_date(ent):\n",
    "    text = ent.text.strip().lower()\n",
    "    # If it's a plain number, be skeptical\n",
    "    if text.isdigit():\n",
    "        num = int(text)\n",
    "        # Ages, counts, room numbers, etc.\n",
    "        if 0 < num <= 30:\n",
    "            # Check surrounding context for non-temporal signals\n",
    "            left = ent.doc[max(ent.start - 2, 0):ent.start]\n",
    "            right = ent.doc[ent.end:min(ent.end + 2, len(ent.doc))]\n",
    "            window = \" \".join([t.text.lower() for t in list(left) + list(right)])\n",
    "            if re.search(r\"(age|year[- ]?old|kid|child|room|group|people|class|seat)\", window):\n",
    "                return True\n",
    "    # Also guard against short dates like \"8\" or \"12\"\n",
    "    if len(text) <= 2 and text.isdigit():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def compute_duration_from_dates(date_range):\n",
    "    start = datetime.strptime(date_range[\"start\"], \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(date_range[\"end\"], \"%Y-%m-%d\")\n",
    "    duration = (end - start).days\n",
    "    return f\"{duration} days\" if duration > 0 else \"Invalid date range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd396f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_relative_day(text):\n",
    "    \"\"\"\n",
    "    Check if the input string refers to a relative day (e.g., today, tomorrow, next week).\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    relative_keywords = [\n",
    "        \"today\", \"tomorrow\", \"tonight\", \"yesterday\",\n",
    "        \"this morning\", \"this evening\", \"this afternoon\",\n",
    "        \"next week\", \"next month\", \"next year\",\n",
    "        \"this week\", \"this month\", \"this year\",\n",
    "        \"coming weekend\", \"this weekend\", \"next weekend\"\n",
    "    ]\n",
    "\n",
    "    # Match relative phrases\n",
    "    for phrase in relative_keywords:\n",
    "        if phrase in text:\n",
    "            return True\n",
    "\n",
    "    # Regex to catch things like \"in 3 days\", \"after 1 week\", \"within 2 months\"\n",
    "    relative_pattern = r\"(in|after|within)\\s+\\d+\\s+(day|days|week|weeks|month|months|year|years)\"\n",
    "    if re.search(relative_pattern, text):\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554da88c-c405-4153-95f4-7ec843057782",
   "metadata": {},
   "source": [
    "### NER Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31f2140a-4cb7-46fb-af5c-887f27a21f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract using spaCy\n",
    "def extract_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    state = init_dialogue_state()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        term = ent.text.strip().lower()\n",
    "        label = ent.label_\n",
    "        \n",
    "        # Handle location\n",
    "        if label in [\"GPE\", \"LOC\"]:\n",
    "            if term not in TRANSPORT_TERMS and term not in state[\"location\"]:\n",
    "                print(term)\n",
    "                state[\"location\"].append(term)\n",
    "            continue\n",
    "            \n",
    "        # Handle DATE vs. duration vs. relative day\n",
    "        if label == \"DATE\":\n",
    "            if is_potential_noise_date(ent):\n",
    "                continue  # skip likely misclassified number\n",
    "            unit = classify_time_entity_fuzzy(ent)\n",
    "            if unit == \"date\":\n",
    "                if is_relative_day(term):\n",
    "                    pass\n",
    "                else:\n",
    "                    norm = normalize_date(term)\n",
    "                    if norm and norm not in state[\"date\"]:\n",
    "                        state[\"date\"].append(norm)\n",
    "            elif unit == \"duration_days\":\n",
    "                if term not in state[\"duration_days\"]:\n",
    "                    state[\"duration_days\"].append(term)\n",
    "            continue  # skip further processing of this term\n",
    "            \n",
    "        # Mapped labels\n",
    "        mapped_label = spacy_to_custom_labels.get(label, None)\n",
    "        if mapped_label and term not in state[mapped_label]:\n",
    "            state[mapped_label].append(term)        \n",
    "        \n",
    "        # Keyword-based overrides\n",
    "        if term in FOOD_TERMS and term not in state[\"food\"]:\n",
    "            state[\"food\"].append(term.lower())\n",
    "        elif term in TRANSPORT_TERMS and term not in state[\"transport\"]:\n",
    "            state[\"transport\"].append(term.lower())\n",
    "        elif term in LOCATION_TERMS and term not in state[\"location\"]:\n",
    "            pass\n",
    "\n",
    "    # Fallback token-level matches\n",
    "    for token in doc:\n",
    "        word = token.lemma_.lower()\n",
    "        \n",
    "        if word in FOOD_TERMS and word not in state[\"food\"]:\n",
    "            state[\"food\"].append(word)\n",
    "        elif word in TRANSPORT_TERMS and word not in state[\"transport\"]:\n",
    "            state[\"transport\"].append(word)\n",
    "        elif word in LOCATION_TERMS and word not in state[\"location\"]:\n",
    "            pass\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b3a502c-f9bb-4c95-a08f-32408f6d598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singapore\n",
      "intent: None\n",
      "location: ['singapore']\n",
      "date: ['2025-07-07']\n",
      "duration_days: []\n",
      "food: []\n",
      "budget: []\n",
      "transport: []\n",
      "event: []\n",
      "style: []\n",
      "num_kids: []\n",
      "num_adults: []\n",
      "special: []\n"
     ]
    }
   ],
   "source": [
    "test = \"We’re interested in Singapore’s local culture and historical sights 7 July, but would prefer simplified names or explanations for easier understanding.\"\n",
    "# print(extract_with_spacy(test))\n",
    "result = extract_with_spacy(test)\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb48b83-8e8f-4748-9d39-598a8256afd4",
   "metadata": {},
   "source": [
    "## Check Missing Essential Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35ddf11c-1edd-4255-aabc-84df577f1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_fields(parsed_response):\n",
    "    missing = []\n",
    "    for field in ESSENTIAL_FIELDS:\n",
    "        value = parsed_response.get(field)\n",
    "        if value is None or (isinstance(value, list) and len(value) == 0):\n",
    "            missing.append(field)\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21275bdf-3a46-4013-be08-a80e6aef40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_or_clarify(user_query):\n",
    "    # Try quick parse from your own rules or previous step\n",
    "    parsed = quick_parse(user_query)  # Assume you have a lightweight parser\n",
    "\n",
    "    missing = missing_fields(parsed)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"🤖 I need more info. Could you please provide: {', '.join(missing)}?\")\n",
    "        return None  # Await user's clarification\n",
    "    else:\n",
    "        return parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cb086-4281-4d29-a4e5-d69236acd42a",
   "metadata": {},
   "source": [
    "## Build Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "924283c8-c101-4907-ae11-cf13a0b4cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_query):\n",
    "    return f\"\"\"\n",
    "You are a travel assistant AI that extracts **only explicitly stated information** from user messages to help plan an itinerary.\n",
    "\n",
    "🔍 Your task is to extract fields from the user query **without guessing or inferring**.  \n",
    "- If something is not clearly mentioned, return `None` (for single values) or `[]` (for lists).  \n",
    "- Normalize any fuzzy or descriptive terms to a known travel-friendly format.\n",
    "\n",
    "Return a valid Python dictionary using this structure:\n",
    "```python\n",
    "dialogue_state = {{\n",
    "    \"location\": List[str],             # Places or landmarks mentioned\n",
    "    \"date\": List[str],                 # Exact or relative dates (e.g., \"June 3\", \"next week\")\n",
    "    \"duration_days\": List[str],        # Durations like \"3 days\", \"a week\"\n",
    "    \"food\": List[str],                 # Local foods, cuisines\n",
    "    \"budget\": List[str],               # Budget phrases like \"$300\", \"under $150\"\n",
    "    \"transport\": List[str],            # Modes of travel: MRT, bus, taxi\n",
    "    \"event\": List[str],                # Activities like shopping, sightseeing, museum\n",
    "    \"style\": List[str],                # Descriptive preferences: relaxed, luxury, tourist-friendly\n",
    "    \"num_kids\": List[str],             # Number of children (if mentioned)\n",
    "    \"num_adults\": List[str],           # Number of adults (if mentioned)\n",
    "    \"special\": List[str]               # Special needs: halal, wheelchair access, baby stroller, others\n",
    "}}\n",
    "\n",
    "🗣 User Query:\n",
    "\"{user_query}\"\n",
    "\n",
    "🔚 Respond with only the dictionary. Do not include explanations, prefixes, or formatting like triple quotes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b2db5-2d94-4fbd-b186-ad22cc2dbdea",
   "metadata": {},
   "source": [
    "## Call LLM when Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfc21f21-e77d-439d-88f4-c7dd6be66ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': None, 'date': None, 'duration_days': None, 'food': None, 'budget': None, 'transport': None, 'event': None, 'style': None, 'num_kids': None, 'num_adults': None, 'special': None}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def call_ollama_mistral(user_query):\n",
    "    prompt = build_prompt(user_query)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": \"mistral\", \"prompt\": prompt},\n",
    "            stream=True,\n",
    "            timeout=30\n",
    "        )\n",
    "    except requests.RequestException as e:\n",
    "        print(\"❌ Ollama request failed:\", e)\n",
    "        return {}\n",
    "\n",
    "    response_text = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                part = json.loads(line)\n",
    "                if \"response\" in part:\n",
    "                    response_text += part[\"response\"]\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    try:\n",
    "        # Remove \"dialogue_state =\" or any prefix\n",
    "        dict_start = response_text.find(\"{\")\n",
    "        dict_str = response_text[dict_start:].strip()\n",
    "\n",
    "        # ✅ Use ast.literal_eval for Python-style literal\n",
    "        parsed = ast.literal_eval(dict_str)\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Ollama dict parse error:\", str(e))\n",
    "        print(\"🔁 Raw response:\", response_text)\n",
    "        return {}\n",
    "\n",
    "# Test ollama mistral    \n",
    "user_query = 'Hello world!'\n",
    "response = call_ollama_mistral(user_query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7e99e-922e-4148-93ef-1583a570f1ad",
   "metadata": {},
   "source": [
    "## Dialogue State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c705d72c-f177-4cf8-b20c-3be3420df110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_states(primary, fallback):\n",
    "    merged = primary.copy()\n",
    "    for key in merged:\n",
    "        is_empty = merged[key] in [None, [], {}]\n",
    "        has_fallback = fallback.get(key) not in [None, [], {}]\n",
    "        if is_empty and has_fallback:\n",
    "            merged[key] = fallback[key]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21164a7f-d3c6-4891-9824-94a61f7584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_itinerary_intent(intent, user_query):\n",
    "    responses = {\n",
    "        \"FindPlace\": \"📍 I can help you find a place! What type of place are you looking for?\",\n",
    "        \"BookFlight\": \"✈️ Sure! I can help you book a flight. When and where do you want to travel?\",\n",
    "        \"AskOpeningHour\": \"⏰ Please tell me which place you'd like to know the opening hours for.\",\n",
    "        \"SearchHotel\": \"🏨 Looking for a hotel? Let me know your destination and budget.\",\n",
    "    }\n",
    "    return responses.get(intent, \"🤖 I'm not sure how to help with that yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f5d45e6-7833-4283-82d9-35c436aea2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_additional_signals(text):\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Direct phrase match\n",
    "    for term in SPECIAL_REQUIREMENT_TERMS:\n",
    "        if term in text_lower:\n",
    "            return True, term\n",
    "\n",
    "    # Regex patterns for common needs\n",
    "    patterns = [\n",
    "        r\"\\b(no\\s+stairs|no\\s+smoking|non[-\\s]?smoking)\\b\",\n",
    "        r\"\\b(kid[-\\s]?friendly|baby[-\\s]?friendly|pet[-\\s]?friendly)\\b\",\n",
    "        r\"\\b(gluten[-\\s]?free|wheelchair[-\\s]?accessible)\\b\",\n",
    "        r\"\\b(halal|kosher|vegetarian|vegan)\\b\",\n",
    "        r\"\\b(avoid\\s+(crowds|crowded))\\b\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return True, pattern\n",
    "\n",
    "    # Fallback: check noun chunks with spaCy\n",
    "    doc = nlp(text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower()\n",
    "        if any(term in chunk_text for term in SPECIAL_REQUIREMENT_TERMS):\n",
    "            return True, chunk_text\n",
    "\n",
    "    return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "439e45cc-2ca3-4ad8-ba1a-d2788193d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "NEW_PLAN_EXAMPLES = [\n",
    "    \"plan a new trip\",\n",
    "    \"start a new itinerary\",\n",
    "    \"create a new travel plan\",\n",
    "    \"begin another journey\",\n",
    "    \"forget the last trip\",\n",
    "    \"make a new plan\",\n",
    "    \"we want to go somewhere else now\",\n",
    "    \"i'm planning a different trip\",\n",
    "    \"start over\",\n",
    "    \"next, I want to plan something new\"\n",
    "]\n",
    "\n",
    "def is_new_plan(user_query, threshold=0.7):\n",
    "    query_emb = st_model.encode(user_query, convert_to_tensor=True)\n",
    "    example_embs = st_model.encode(NEW_PLAN_EXAMPLES, convert_to_tensor=True)\n",
    "\n",
    "    similarity_scores = util.cos_sim(query_emb, example_embs)[0]\n",
    "    max_score = float(similarity_scores.max())\n",
    "\n",
    "    return max_score > threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2f122-1a03-4752-870f-d1e00ee2f0b8",
   "metadata": {},
   "source": [
    "## Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "028ff261-85d1-4b51-a5fb-66a6b4f6e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_CONFIRM_PHRASES = {\n",
    "    \"looks good\", \"that's fine\", \"okay\", \"confirm\", \"yes\", \"that works\",\n",
    "    \"good to go\", \"done\", \"finalize\", \"proceed\", \"complete the plan\"\n",
    "}\n",
    "\n",
    "def is_final_confirmation(text):\n",
    "    return text.strip().lower() in FINAL_CONFIRM_PHRASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bd4600f-3d99-4e6a-a985-6fa9155486c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_PHRASES = {\"yes\", \"yeah\", \"sure\", \"of course\", \"yep\", \"affirmative\", \"let's go\", \"ok\", \"okay\"}\n",
    "\n",
    "def is_affirmative(text):\n",
    "    return text.strip().lower() in YES_PHRASES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53a02d",
   "metadata": {},
   "source": [
    "## RAG Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fd5386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore is the world's third-largest financial center after New York City and London.\n",
      "The Merlion, a mythical creature with a lion's head and the body of a fish, is the mascot and national personification of Singapore.\n",
      "Gardens by the Bay, a 101-hectare park in central Singapore that consists of three waterfront gardens, is home to Supertree Grove, iconic vertical gardens up to 50 meters tall.\n"
     ]
    }
   ],
   "source": [
    "def sent_prompt_to_llm(prompt):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": \"mistral\", \"prompt\": prompt},\n",
    "            stream=True,\n",
    "            timeout=30\n",
    "        )\n",
    "    except requests.RequestException as e:\n",
    "        print(\"❌ Ollama request failed:\", e)\n",
    "        return {}\n",
    "\n",
    "    # Stream and collect response chunks\n",
    "    output = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line.decode(\"utf-8\"))\n",
    "                output += data.get(\"response\", \"\")\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Ignore malformed lines\n",
    "\n",
    "    return output\n",
    "\n",
    "# rag vector db simulator to generate factual context based on similarity\n",
    "import json\n",
    "def rag_vdb_sim(fact_count):\n",
    "    if fact_count <= 0:\n",
    "        return []\n",
    "    elif fact_count > 3:\n",
    "        fact_count = 3 # keep max at 3 to reduce tokens and speed up the response.\n",
    "    \n",
    "    response = sent_prompt_to_llm(\n",
    "        f\"You are a travel domain assistant. Generate {fact_count} factual travel-related information about Singapore. \"\n",
    "        \"Return the output strictly as a JSON array of strings, where each array element contains one fact. \"\n",
    "        \"Do not include any explanations or formatting outside the array.\"\n",
    "    )\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse response into JSON array.\", response)\n",
    "        return []    \n",
    "\n",
    "rag_chunks = rag_vdb_sim(3)\n",
    "for chunk in rag_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5963ee",
   "metadata": {},
   "source": [
    "## Persona Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1144ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adventure Seeker', 'Family Traveler']\n",
      "['Adventure Seeker', 'Family Traveler']\n",
      "[]\n",
      "['Adventure Seeker', 'Luxury Seeker', 'Relaxation Seeker']\n",
      "['Solo Traveler']\n",
      "['Family Traveler']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, AutoModelForSequenceClassification, RobertaTokenizerFast\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer exactly the same as training\n",
    "persona_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "persona_model = AutoModelForSequenceClassification.from_pretrained(\"./bert_multilabel_persona/checkpoint-2155\")\n",
    "persona_label_encoder = joblib.load(\"./bert_multilabel_persona/label_encoder.bin\")  # This is a MultiLabelBinarizer\n",
    "\n",
    "# persona_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "# persona_model = AutoModelForSequenceClassification.from_pretrained(\"./roberta_multilabel_persona/checkpoint-2155\")\n",
    "# persona_label_encoder = joblib.load(\"./roberta_multilabel_persona/label_encoder.bin\")  # This is a MultiLabelBinarizer\n",
    "\n",
    "persona_model.eval()\n",
    "\n",
    "def predict_personas(user_query, threshold=0.5):\n",
    "    inputs = persona_tokenizer(user_query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = persona_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.sigmoid(logits).squeeze().numpy()  # Convert logits to probabilities\n",
    "\n",
    "    # Predict all classes above the threshold\n",
    "    predicted_labels = np.where(probs >= threshold)[0]\n",
    "    personas = persona_label_encoder.classes_[predicted_labels]\n",
    "    return list(personas)\n",
    "\n",
    "\n",
    "# Test\n",
    "user_queries = [\n",
    "    \"We're 6 young adults (25-30) staying in Singapore for 6 days. We love outdoor activities, hiking trails, cycling, and unique experiences like night safaris. Include one rest day and show us images of adventure activities available.\",\n",
    "    \"A family with a child in a wheelchair, maximize sightseeing in 3 days in Singapore\",\n",
    "    \"We're tech conference attendees, maximize sightseeing in 3 days in Singapore with MRT travel\",\n",
    "    \"We want a wellness retreat, maximize sightseeing in 3 days in Singapore for a weekend\",\n",
    "    \"I'm a solo traveler, experience something unique in Singapore (from July 1 to July 5) avoiding crowded places\",\n",
    "    \"A couple planning a honeymoon, experience something unique in Singapore (sometime in March) for a weekend including vegan options\",\n",
    "]\n",
    "for query in user_queries:\n",
    "    personas = predict_personas(query, threshold=0.5)\n",
    "    print(personas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd0c05-872b-4471-bff9-433829e9f95b",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a724bb0-1d57-492a-8aaa-81a1cbd74105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_gen import generate_prompt, generate_response\n",
    "\n",
    "def process_user_input(user_query, dialogue_state=None, plan_finalized=False):\n",
    "    \"\"\"\n",
    "    Travel chatbot processor for PlanItinerary (single intent only).\n",
    "    \n",
    "    Features:\n",
    "    - Detects if user wants to start a new plan after one was proposed.\n",
    "    - Handles structured dialogue (AI Agent / User).\n",
    "    - Extracts essential fields via spaCy.\n",
    "    - Calls LLM for additional details if signals found.\n",
    "    \"\"\"\n",
    "    # Step 0: Initialize state if needed\n",
    "    if dialogue_state is None:\n",
    "        dialogue_state = init_dialogue_state()\n",
    "        plan_finalized = False\n",
    "       \n",
    "    # Step 1: Handle post-finalization message\n",
    "    if plan_finalized:\n",
    "        if is_new_plan(user_query) or is_affirmative(user_query):\n",
    "            dialogue_state = init_dialogue_state()\n",
    "            plan_finalized = False\n",
    "            return dialogue_state, \"🆕 Got it! Starting a new itinerary. Let's begin.\", plan_finalized\n",
    "        else:\n",
    "            return dialogue_state, (\n",
    "                \"🤖 Would you like to plan a new itinerary? \"\n",
    "                \"You can say something like 'Plan a new trip to Tokyo'.\"\n",
    "            ), plan_finalized\n",
    "\n",
    "    # Step 2: Classify intent if not set\n",
    "    if dialogue_state[\"intent\"] is None:\n",
    "        intent = classify_intent(user_query)\n",
    "        if intent == \"PlanItinerary\":\n",
    "            dialogue_state[\"intent\"] = intent\n",
    "        else:\n",
    "            return dialogue_state, \"❌ I can only help with itinerary planning for now.\", plan_finalized\n",
    "\n",
    "    # Step 3: Extract essential fields with spaCy\n",
    "    spacy_state = extract_with_spacy(user_query)\n",
    "    dialogue_state = merge_states(dialogue_state, spacy_state)\n",
    "\n",
    "    # Only infer duration if not already provided explicitly #HACK\n",
    "    # if not dialogue_state[\"duration_days\"] and len(dialogue_state[\"date\"]) >= 2:\n",
    "    #     duration_str = compute_duration_from_dates(dialogue_state[\"date\"])\n",
    "    #     if duration_str:\n",
    "    #         dialogue_state[\"duration_days\"].append(duration_str)\n",
    "\n",
    "    # Step 4: Check for missing essential fields\n",
    "    essential_fields = [\"intent\"]\n",
    "    # essential_fields = [\"intent\",\"location\"]\n",
    "    # essential_fields = [\"location\", \"date\"]\n",
    "    # essential_fields = [\"location\", \"date\", \"duration_days\"]\n",
    "    missing = [key for key in essential_fields if not dialogue_state[key]]\n",
    "\n",
    "    if missing:\n",
    "        return dialogue_state, (\n",
    "            f\"⚠️ I still need the following: {', '.join(missing)}.\\n\"\n",
    "            \"Could you please provide that?\"\n",
    "        ), plan_finalized\n",
    "\n",
    "    #Step 5: Extract additional info if signals detected \n",
    "    if detect_additional_signals(user_query):\n",
    "        llm_state = call_ollama_mistral(user_query)\n",
    "        dialogue_state = merge_states(dialogue_state, llm_state)\n",
    "    \n",
    "    # Step 6: Finalize if user confirms\n",
    "    if is_final_confirmation(user_query):\n",
    "        plan_finalized = True\n",
    "        response_msg = (\n",
    "            \"🎉 Great! Your itinerary has been finalized:\\n\"\n",
    "            f\"```json\\n{json.dumps(dialogue_state, indent=2)}\\n```\\n\"\n",
    "            \"Would you like to plan another trip?\"\n",
    "        )\n",
    "        return dialogue_state, response_msg, plan_finalized\n",
    "\n",
    "    # Step 7: Detect Persona\n",
    "    dialogue_state['persona'] = predict_personas(user_query, threshold=0.5)\n",
    "\n",
    "    # Step 8: Generate prompt\n",
    "    # rag_chunks = rag_vdb_sim(3)\n",
    "    rag_chunks = rag_vdb_sim(0)\n",
    "    prompt = generate_prompt(user_query, dialogue_state, rag_chunks, 200)\n",
    "    # return dialogue_state, prompt\n",
    "\n",
    "    # Step 9: Send prompt to LLM to get travel advice\n",
    "    # travel_advice = sent_prompt_to_llm(prompt)\n",
    "    travel_response = generate_response(prompt)\n",
    "\n",
    "\n",
    "    # # Step 10: Show proposed plan and wait for confirmation\n",
    "    response_msg = (\n",
    "        \"✅ Here's your current itinerary summary:\\n\"\n",
    "        f\"{travel_response} \\n\\n\"\n",
    "        f\"```json\\n{json.dumps(dialogue_state, indent=2)}\\n```\\n\"\n",
    "        \"Would you like to add or modify anything?\\n\"\n",
    "        \"Say 'looks good' or 'confirm' to finalize.\"\n",
    "    )\n",
    "    return dialogue_state, response_msg, plan_finalized, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f3362b0-1b96-48db-be67-82e9574ad2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'intent': 'PlanItinerary',\n",
       "  'location': ['None'],\n",
       "  'date': ['2025-06-07'],\n",
       "  'duration_days': ['7 days'],\n",
       "  'food': ['traditional food experiences'],\n",
       "  'budget': ['None'],\n",
       "  'transport': ['None'],\n",
       "  'event': ['activities suitable for all ages',\n",
       "   'accessible attractions',\n",
       "   'family-friendly entertainment'],\n",
       "  'style': ['None'],\n",
       "  'num_kids': ['None'],\n",
       "  'num_adults': ['8'],\n",
       "  'special': ['accessible attractions'],\n",
       "  'persona': ['Family Traveler']},\n",
       " \"You are a highly experienced travel expert and advisor for Singapore. Your task is to provide a well-structured and practical travel itinerary based on the user's needs.\\n\\nHere is the user's original request:\\n We're a group of 8 spanning three generations (grandparents, parents, teens) visiting for 7 days. Need activities suitable for all ages including accessible attractions, traditional food experiences, and family-friendly entertainment. Plan one rest day mid-week.\\n\\nSpecifically, as a Family Traveler , the user wants to do PlanItinerary The travel dates are 2025-06-07. The trip duration is 7 days days. The user prefers traditional food experiences cuisine. The user is interested in events like activities suitable for all ages, accessible attractions, family-friendly entertainment. Traveling group includes 8 adults. Special preferences include accessible attractions.\\n\\nInstructions:\\n 1. Keep the response concise, engaging and aligned with the user's intent and preferences.\\n2. Maintain a helpful, friendly and expert tone.\\n3. The response must not exceed 200 words.\\n\\n\\nNow, please generate an itinerary with one relaxing day to rest in the middle.\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"We are a family of four—two adults and two kids aged 5 and 8—planning a trip to Singapore.\"\n",
    "user_query = \"We are a family of four—two adults and two kids aged 5 and 8—planning a trip to Singapore in 6 July for 3 days.\"\n",
    "user_query = \"We're a family of 4 with two children aged 6 and 9 visiting Singapore for 5 days. We love interactive science exhibits, nature parks, and kid-friendly activities. Can you suggest an itinerary with one rest day in the middle? Show us relevant attractions with images.\"\n",
    "user_query = \"We're 6 young adults (25-30) staying in Singapore for 6 days. We love outdoor activities, hiking trails, cycling, and unique experiences like night safaris. Include one rest day and show us images of adventure activities available.\"\n",
    "user_query = \"We're a group of 8 spanning three generations (grandparents, parents, teens) visiting for 7 days. Need activities suitable for all ages including accessible attractions, traditional food experiences, and family-friendly entertainment. Plan one rest day mid-week.\"\n",
    "process_user_input(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a980495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            RawQuery  StructuredQuery\n",
      "0  We're a family of 4 with two children aged 6 a...              NaN\n",
      "1  My husband and I (both 65+) are visiting Singa...              NaN\n",
      "2  We're 6 young adults (25-30) staying in Singap...              NaN\n",
      "3  Three colleagues extending our business trip f...              NaN\n",
      "4  We're a group of 8 spanning three generations ...              NaN\n",
      "singapore\n",
      "singapore\n",
      "little india\n",
      "singapore\n",
      "gardens by the bay\n",
      "marina bay\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "laksa\n",
      "kampong gam\n",
      "marina bay\n",
      "marina bay\n",
      "sentosa\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "⚠️ Ollama dict parse error: malformed node or string on line 10: <ast.Call object at 0x39fdbe8c0>\n",
      "🔁 Raw response:  {\n",
      "    \"location\": [\"Singapore\"],\n",
      "    \"date\": None,\n",
      "    \"duration_days\": [\"3 days\"],\n",
      "    \"food\": [],\n",
      "    \"budget\": None,\n",
      "    \"transport\": [],\n",
      "    \"event\": [\"kid-friendly\"],\n",
      "    \"style\": [],\n",
      "    \"num_kids\": [str(2)],\n",
      "    \"num_adults\": [str(2)],\n",
      "    \"special\": []\n",
      "}\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "sentosa\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "⚠️ Ollama dict parse error: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 3)\n",
      "🔁 Raw response:  {\n",
      "    \"location\": [\"Singapore\"],\n",
      "    \"food\": [\"local cuisine\"]\n",
      "    \"num_adults\": [\"2\"]  # assuming grandparents are adults\n",
      "}\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "sentosa\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "sentosa\n",
      "gardens by the bay\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "sentosa\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n",
      "singapore\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# df = pd.read_csv(\"user_queries1.csv\")\n",
    "df = pd.read_csv(\"user_queries.csv\")\n",
    "print(df.head())\n",
    "\n",
    "df['StructuredQuery'] = df['RawQuery'].apply(lambda q: process_user_input(q)[1]) #the fourth value in the returned tuple is the prompt_template\n",
    "# df['StructuredQuery'] = df['StructuredQuery'].apply(lambda x: f'\"{x}\"')\n",
    "\n",
    "# df.to_csv(\"structured_queries.csv\", index=False, quoting=0)  # quoting=0 means no automatic quoting\n",
    "df.to_csv(\"structured_queries_persona_bert.csv\", index=False, quoting=0)  # quoting=0 means no automatic quoting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6eb96",
   "metadata": {},
   "source": [
    "# Chatbot Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cf1d21ef-4397-4116-bf2a-361edf7b072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f9/r9r71s191dg67f1mbgyvj4qh0000gn/T/ipykernel_73847/4203826595.py:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7905\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7905/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singapore\n",
      "Failed to parse response into JSON array.  [\n",
      "  \"Singapore is the world's only island city-state, located at the tip of the Malay Peninsula in Southeast Asia.\",\n",
      "  \"The Singapore Zoo, known as Mandai Park, is renowned for its open-concept exhibits and has been named the best rainforest zoo in the world by BBC Wildlife Magazine.\",\n",
      "  \"Raffles Hotel, a colonial-style luxury hotel, was established in 1887 and is where the Singapore Sling cocktail was first served.\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_fn(message, history, state_bundle):\n",
    "    dialogue_state, plan_finalized = state_bundle\n",
    "\n",
    "    # Process input\n",
    "    updated_state, reply, plan_finalized = process_user_input(message, dialogue_state, plan_finalized)\n",
    "\n",
    "    # Append to history\n",
    "    history.append((f\"User: {message}\", f\"🤖AI Agent: {reply}\"))\n",
    "\n",
    "    return history, [updated_state, plan_finalized], \"\"  # Update both in Gradio state\n",
    "\n",
    "CSS =\"\"\"\n",
    ".contain { display: flex; flex-direction: column; }\n",
    "#chatbot { flex-grow: 1; }\n",
    "#component-0 { height: 100%; }\n",
    "#component-1 { height: 800px; }\n",
    "\"\"\"\n",
    "with gr.Blocks(css=CSS) as demo:\n",
    "    gr.Markdown(\"# Personal AI Travel Agent (PAT)\")           # Visible big title in UI\n",
    "    gr.Markdown(\"Where do you want to go today?\")  # Description text\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        elem_id=\"chatbot\",\n",
    "        height=\"100vh\",\n",
    "    )\n",
    "    state = gr.State([init_dialogue_state(), False])  # 🧠 Bundle: [dialogue_state, plan_finalized]\n",
    "    history = gr.State([])\n",
    "    finalized_flag = gr.State(False)\n",
    "    msg = gr.Textbox(label=\"Type your message here\")\n",
    "    msg.submit(\n",
    "        fn=chat_fn,\n",
    "        inputs=[msg, history, state],\n",
    "        outputs=[chatbot, state, msg]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22293c55-636d-499d-868e-e6752e5ec1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "sample_input = [\n",
    "    \"I want to plan an itinerary that includes laksa, Kampong Gam and Marina Bay, and maybe take the MRT from 3 June 2025 to 9 June 2025.\",\n",
    "    \"Plan a 3-day trip around Marina Bay and Sentosa, eat laksa, take the MRT, and spend under $150. On Day 2 we want to shop.\",    \n",
    "    \"We are a family of four—two adults and two kids aged 5 and 8—planning a trip to Singapore.\",\n",
    "    \"We'll be in Singapore for 6 days including arrival and departure, so we have 4 full days to explore.\",\n",
    "    \"Please suggest daytime activities in Singapore that are fun and creative for young children.\",\n",
    "    \"After our kids go to bed, we’d like some recommendations for fun and fashionable adult activities at night.\",\n",
    "    \"We’re interested in Singapore’s local culture and historical sights, but would prefer simplified names or explanations for easier understanding.\",\n",
    "    \"We prefer a relaxed itinerary: maximum two attractions per day, with one rest day in the middle to explore freely.\",\n",
    "    \"Please generate a 6-day itinerary for our family trip to Singapore, considering only 4 full days for activities.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f743cc7-cf41-42ab-994c-d27e375744a8",
   "metadata": {},
   "source": [
    "## Output the NER Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5caa39-c972-4b35-9c77-0af5b4977fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_dialogue_state = process_user_input(sample_input)\n",
    "for s in sample_input:\n",
    "    test = understand_entities(s)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0d58d-63ed-4676-bb8b-e8e5c0d18e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(sample_input):\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Sample: {i + 1}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dialogue_state = init_dialogue_state()\n",
    "    sample_output = process_user_input(s, dialogue_state)\n",
    "    print(sample_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
